name: Healthcare Management System CI/CD - RASM Compliant

on:
  push:
    branches: [ main, develop, feature/* ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    - cron: '0 2 * * *'  # Nightly build at 2 AM
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target environment'
        required: true
        default: 'development'
        type: choice
        options:
        - development
        - staging
        - production
      test_suite:
        description: 'Test suite to run'
        required: false
        default: 'all'
        type: choice
        options:
        - all
        - unit
        - integration
        - e2e
        - performance
        - security
      skip_deployment:
        description: 'Skip deployment step'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  STREAMLIT_SERVER_PORT: 8501
  STREAMLIT_SERVER_ADDRESS: 0.0.0.0
  RASM_COMPLIANCE_THRESHOLD: 0.85
  COVERAGE_THRESHOLD: 85
  PERFORMANCE_THRESHOLD_MS: 2000

jobs:
  # Pre-flight checks for reliability
  preflight:
    name: Pre-flight Checks
    runs-on: ubuntu-latest
    outputs:
      should_run_tests: ${{ steps.check.outputs.should_run }}
      test_matrix: ${{ steps.matrix.outputs.matrix }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        
    - name: Check if tests should run
      id: check
      run: |
        if [[ "${{ github.event_name }}" == "schedule" ]] || [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
          echo "should_run=true" >> $GITHUB_OUTPUT
        elif git diff --name-only ${{ github.event.before }}..${{ github.sha }} | grep -E '\.(py|yml|yaml|json|toml|cfg|ini)$'; then
          echo "should_run=true" >> $GITHUB_OUTPUT
        else
          echo "should_run=false" >> $GITHUB_OUTPUT
        fi
        
    - name: Generate test matrix
      id: matrix
      run: |
        if [[ "${{ github.event.inputs.test_suite }}" == "all" ]] || [[ -z "${{ github.event.inputs.test_suite }}" ]]; then
          echo 'matrix={"suite":["lint","security","unit","integration","e2e","performance"]}' >> $GITHUB_OUTPUT
        else
          echo 'matrix={"suite":["${{ github.event.inputs.test_suite }}"]}' >> $GITHUB_OUTPUT
        fi

  # Code quality analysis for maintainability
  code-quality:
    name: Code Quality Analysis
    runs-on: ubuntu-latest
    needs: preflight
    if: needs.preflight.outputs.should_run_tests == 'true'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install black isort flake8 mypy bandit safety pytest-cov
        
    - name: Create artifacts directory
      run: mkdir -p artifacts/reports
        
    - name: Run comprehensive code quality checks
      run: |
        python execute_tests.py --suites lint type_check format_check import_sort --parallel
        
    - name: Upload code quality reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: code-quality-reports
        path: artifacts/reports/
        retention-days: 30

  # Security scanning for reliability and availability
  security-scan:
    name: Security Analysis
    runs-on: ubuntu-latest
    needs: preflight
    if: needs.preflight.outputs.should_run_tests == 'true'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install bandit safety semgrep
        
    - name: Create artifacts directory
      run: mkdir -p artifacts/reports
        
    - name: Run security analysis
      run: |
        python execute_tests.py --suites security dependency_check --parallel
        
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: artifacts/reports/
        retention-days: 30

  # Comprehensive test suite with RASM validation
  test-suite:
    name: Test Suite (${{ matrix.suite }})
    runs-on: ubuntu-latest
    needs: [preflight, code-quality]
    if: needs.preflight.outputs.should_run_tests == 'true'
    
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.preflight.outputs.test_matrix) }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-xdist pytest-mock pytest-benchmark pytest-timeout
        
    - name: Set up test environment
      run: |
        mkdir -p artifacts/reports
        mkdir -p test-results
        export PYTHONPATH="$PWD/src:$PYTHONPATH"
        
    - name: Run ${{ matrix.suite }} tests
      run: |
        python execute_tests.py --suites ${{ matrix.suite }} --output-dir artifacts/reports
        
    - name: Validate RASM compliance
      if: matrix.suite == 'unit' || matrix.suite == 'integration'
      run: |
        python ci_automation.py --validate-rasm --threshold ${{ env.RASM_COMPLIANCE_THRESHOLD }}
        
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.suite }}
        path: |
          artifacts/reports/
          test-results/
        retention-days: 30
        
    - name: Upload coverage to Codecov
      if: matrix.suite == 'unit' || matrix.suite == 'integration'
      uses: codecov/codecov-action@v3
      with:
        file: ./artifacts/reports/coverage.xml
        flags: ${{ matrix.suite }}
        name: codecov-${{ matrix.suite }}
        fail_ci_if_error: false

  # RASM compliance validation
  rasm-compliance:
    name: RASM Compliance Check
    runs-on: ubuntu-latest
    needs: [test-suite, security-scan]
    if: always() && needs.preflight.outputs.should_run_tests == 'true'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Download all test artifacts
      uses: actions/download-artifact@v3
      with:
        path: artifacts/
        
    - name: Run RASM compliance validation
      run: |
        python ci_automation.py --validate-rasm --comprehensive --threshold ${{ env.RASM_COMPLIANCE_THRESHOLD }}
        
    - name: Generate RASM compliance report
      run: |
        python test_automation.py --generate-rasm-report --output artifacts/rasm-compliance.json
        
    - name: Upload RASM compliance report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: rasm-compliance-report
        path: artifacts/rasm-compliance.json
        retention-days: 90
        
    - name: Comment RASM score on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          try {
            const report = JSON.parse(fs.readFileSync('artifacts/rasm-compliance.json', 'utf8'));
            const score = report.overall_score || 0;
            const threshold = ${{ env.RASM_COMPLIANCE_THRESHOLD }};
            const status = score >= threshold ? '‚úÖ PASSED' : '‚ùå FAILED';
            
            const comment = `## RASM Compliance Report ${status}
            
            **Overall RASM Score:** ${score.toFixed(2)} / 1.00
            **Threshold:** ${threshold}
            
            ### Component Scores:
            - **Reliability:** ${(report.reliability_score || 0).toFixed(2)}
            - **Availability:** ${(report.availability_score || 0).toFixed(2)}
            - **Scalability:** ${(report.scalability_score || 0).toFixed(2)}
            - **Maintainability:** ${(report.maintainability_score || 0).toFixed(2)}
            
            ${score < threshold ? '‚ö†Ô∏è **Action Required:** RASM compliance below threshold. Please review and improve before merging.' : 'üéâ **Great job!** RASM compliance meets requirements.'}`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          } catch (error) {
            console.log('Could not read RASM report:', error.message);
          }

  # Performance and load testing for scalability
  performance-tests:
    name: Performance & Load Tests
    runs-on: ubuntu-latest
    needs: [test-suite]
    if: needs.preflight.outputs.should_run_tests == 'true' && (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' || contains(github.event.head_commit.message, '[perf-test]'))
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install locust pytest-benchmark
        
    - name: Run performance tests
      run: |
        python execute_tests.py --suites performance --output-dir artifacts/reports
        
    - name: Validate performance thresholds
      run: |
        python -c "
        import json
        import sys
        try:
            with open('artifacts/reports/performance_results.json', 'r') as f:
                results = json.load(f)
            avg_response_time = results.get('average_response_time_ms', 0)
            threshold = ${{ env.PERFORMANCE_THRESHOLD_MS }}
            if avg_response_time > threshold:
                print(f'‚ùå Performance test failed: {avg_response_time}ms > {threshold}ms')
                sys.exit(1)
            else:
                print(f'‚úÖ Performance test passed: {avg_response_time}ms <= {threshold}ms')
        except FileNotFoundError:
            print('‚ö†Ô∏è Performance results not found, skipping validation')
        "
        
    - name: Upload performance reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-reports
        path: artifacts/reports/
        retention-days: 30

  # Build and package for deployment
  build:
    name: Build Application
    runs-on: ubuntu-latest
    needs: [rasm-compliance, performance-tests]
    if: always() && (needs.rasm-compliance.result == 'success' || needs.rasm-compliance.result == 'skipped')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install build wheel setuptools
        
    - name: Create version info
      run: |
        echo "BUILD_VERSION=${GITHUB_SHA::8}" >> $GITHUB_ENV
        echo "BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ')" >> $GITHUB_ENV
        echo "BUILD_BRANCH=${GITHUB_REF#refs/heads/}" >> $GITHUB_ENV
        
    - name: Build application package
      run: |
        python -m build
        
    - name: Create deployment package
      run: |
        mkdir -p deployment-package
        cp -r src/ deployment-package/
        cp requirements.txt deployment-package/
        cp *.py deployment-package/ 2>/dev/null || true
        cp -r config/ deployment-package/ 2>/dev/null || true
        
        # Create deployment info
        cat > deployment-package/deployment-info.json << EOF
        {
          "version": "$BUILD_VERSION",
          "build_date": "$BUILD_DATE",
          "branch": "$BUILD_BRANCH",
          "commit": "$GITHUB_SHA",
          "workflow_run": "$GITHUB_RUN_ID"
        }
        EOF
        
        tar -czf healthcare-app-$BUILD_VERSION.tar.gz -C deployment-package .
    
    - name: Upload build artifacts
      uses: actions/upload-artifact@v3
      with:
        name: build-artifacts
        path: |
          dist/
          healthcare-app-*.tar.gz
          deployment-package/deployment-info.json
        retention-days: 30

  # Deploy to development environment
  deploy-dev:
    name: Deploy to Development
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/develop' && github.event_name == 'push'
    environment:
      name: development
      url: https://dev-healthcare.example.com
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download build artifacts
      uses: actions/download-artifact@v3
      with:
        name: build-artifacts
        path: artifacts/
        
    - name: Deploy to development
      run: |
        echo "üöÄ Deploying to development environment..."
        # Add your deployment commands here
        # Example: kubectl apply -f k8s/dev/ or docker-compose up -d
        
    - name: Run smoke tests
      run: |
        python execute_tests.py --suites smoke --environment development
        
    - name: Notify deployment status
      if: always()
      run: |
        echo "‚úÖ Development deployment completed"

  # Deploy to staging environment
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [build, deploy-dev]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment:
      name: staging
      url: https://staging-healthcare.example.com
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download build artifacts
      uses: actions/download-artifact@v3
      with:
        name: build-artifacts
        path: artifacts/
        
    - name: Deploy to staging
      run: |
        echo "üöÄ Deploying to staging environment..."
        # Add your deployment commands here
        
    - name: Run integration tests on staging
      run: |
        python execute_tests.py --suites integration e2e --environment staging
        
    - name: Validate RASM metrics in staging
      run: |
        python ci_automation.py --validate-rasm --environment staging --threshold ${{ env.RASM_COMPLIANCE_THRESHOLD }}

  # Deploy to production environment
  deploy-prod:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [deploy-staging]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push' && !github.event.inputs.skip_deployment
    environment:
      name: production
      url: https://healthcare.example.com
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download build artifacts
      uses: actions/download-artifact@v3
      with:
        name: build-artifacts
        path: artifacts/
        
    - name: Pre-deployment health check
      run: |
        echo "üîç Running pre-deployment health checks..."
        python ci_automation.py --health-check --environment production
        
    - name: Deploy to production with blue-green strategy
      run: |
        echo "üöÄ Deploying to production environment..."
        # Implement blue-green deployment strategy
        # Add your deployment commands here
        
    - name: Post-deployment validation
      run: |
        echo "‚úÖ Running post-deployment validation..."
        python execute_tests.py --suites smoke health_check --environment production
        
    - name: Monitor RASM metrics
      run: |
        python ci_automation.py --monitor-rasm --environment production --duration 300

  # Notification and reporting
  notify:
    name: Notify Results
    runs-on: ubuntu-latest
    needs: [rasm-compliance, performance-tests, build, deploy-prod]
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download all artifacts
      uses: actions/download-artifact@v3
      with:
        path: artifacts/
        
    - name: Generate comprehensive report
      run: |
        python test_automation.py --generate-comprehensive-report --output artifacts/final-report.json
        
    - name: Send notification
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Collect results from all jobs
          const results = {
            workflow_status: '${{ job.status }}',
            rasm_compliance: '${{ needs.rasm-compliance.result }}',
            performance_tests: '${{ needs.performance-tests.result }}',
            build: '${{ needs.build.result }}',
            deployment: '${{ needs.deploy-prod.result }}'
          };
          
          const overallStatus = Object.values(results).every(status => 
            status === 'success' || status === 'skipped'
          ) ? '‚úÖ SUCCESS' : '‚ùå FAILED';
          
          const summary = `## CI/CD Pipeline Results ${overallStatus}
          
          **Workflow:** ${context.workflow}
          **Commit:** ${context.sha.substring(0, 8)}
          **Branch:** ${context.ref.replace('refs/heads/', '')}
          **Triggered by:** ${context.eventName}
          
          ### Job Results:
          - **RASM Compliance:** ${results.rasm_compliance === 'success' ? '‚úÖ' : '‚ùå'} ${results.rasm_compliance}
          - **Performance Tests:** ${results.performance_tests === 'success' ? '‚úÖ' : '‚ùå'} ${results.performance_tests}
          - **Build:** ${results.build === 'success' ? '‚úÖ' : '‚ùå'} ${results.build}
          - **Deployment:** ${results.deployment === 'success' ? '‚úÖ' : '‚ùå'} ${results.deployment}
          
          ### Links:
          - [Workflow Run](${context.payload.repository.html_url}/actions/runs/${context.runId})
          - [Commit](${context.payload.repository.html_url}/commit/${context.sha})
          `;
          
          console.log('Pipeline Summary:', summary);
          
          // Create issue for failed pipelines on main branch
          if (overallStatus.includes('FAILED') && context.ref === 'refs/heads/main') {
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `üö® CI/CD Pipeline Failed - ${context.sha.substring(0, 8)}`,
              body: summary + '\n\n**Action Required:** Please investigate and fix the pipeline failures.',
              labels: ['ci/cd', 'bug', 'high-priority']
            });
          }
          
    - name: Upload final report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: final-pipeline-report
        path: artifacts/final-report.json
        retention-days: 90